{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_character.ipynb  DBP_wiki_data.csv  README.md   labels_index.pkl  texts.pkl\n",
      "CNN_text.ipynb       \u001b[0m\u001b[01;34mGLOVE\u001b[0m/             labels.pkl  \u001b[01;34mnewsgroups\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('DBP_wiki_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>l3</th>\n",
       "      <th>wiki_name</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The 1994 Mindoro earthquake occurred on Novemb...</td>\n",
       "      <td>Event</td>\n",
       "      <td>NaturalEvent</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>1994_Mindoro_earthquake</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The 1917 Bali earthquake occurred at 06:50 loc...</td>\n",
       "      <td>Event</td>\n",
       "      <td>NaturalEvent</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>1917_Bali_earthquake</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The 1941 Colima earthquake occurred on April 1...</td>\n",
       "      <td>Event</td>\n",
       "      <td>NaturalEvent</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>1941_Colima_earthquake</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 1983 Coalinga earthquake occurred on May 2...</td>\n",
       "      <td>Event</td>\n",
       "      <td>NaturalEvent</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>1983_Coalinga_earthquake</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 2013 Bushehr earthquake occurred with a mo...</td>\n",
       "      <td>Event</td>\n",
       "      <td>NaturalEvent</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>2013_Bushehr_earthquake</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     l1            l2  \\\n",
       "0  The 1994 Mindoro earthquake occurred on Novemb...  Event  NaturalEvent   \n",
       "1  The 1917 Bali earthquake occurred at 06:50 loc...  Event  NaturalEvent   \n",
       "2  The 1941 Colima earthquake occurred on April 1...  Event  NaturalEvent   \n",
       "3  The 1983 Coalinga earthquake occurred on May 2...  Event  NaturalEvent   \n",
       "4  The 2013 Bushehr earthquake occurred with a mo...  Event  NaturalEvent   \n",
       "\n",
       "           l3                 wiki_name  word_count  \n",
       "0  Earthquake   1994_Mindoro_earthquake          59  \n",
       "1  Earthquake      1917_Bali_earthquake          68  \n",
       "2  Earthquake    1941_Colima_earthquake         194  \n",
       "3  Earthquake  1983_Coalinga_earthquake          98  \n",
       "4  Earthquake   2013_Bushehr_earthquake          61  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342781, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 342781 entries, 0 to 342780\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   text        342781 non-null  object\n",
      " 1   l1          342781 non-null  object\n",
      " 2   l2          342781 non-null  object\n",
      " 3   l3          342781 non-null  object\n",
      " 4   wiki_name   342781 non-null  object\n",
      " 5   word_count  342781 non-null  int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 15.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text          0\n",
       "l1            0\n",
       "l2            0\n",
       "l3            0\n",
       "wiki_name     0\n",
       "word_count    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Agent',\n",
       " 'Device',\n",
       " 'Event',\n",
       " 'Place',\n",
       " 'Species',\n",
       " 'SportsSeason',\n",
       " 'TopicalConcept',\n",
       " 'UnitOfWork',\n",
       " 'Work'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data['l1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 1994 Mindoro earthquake occurred on November 15 at 03:15 local time near Mindoro, the Philippines. It had a moment magnitude of 7.1. It is associated with a 35 kilometer-long ground rupture, called the Aglubang River fault. Seventy eight people were reported dead, and 7,566 houses were damaged. The earthquake generated a tsunami and landslides on the Verde Island.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet=\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={'alphabet':alphabet,'number_of_characters':len(alphabet),'max_length':150,'dropout_input':0.3,'number_of_classes':len(set(data['l1']))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alphabet': 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-,;.!?:\\'\"/\\\\|_@#$%^&*~`+-=<>()[]{}',\n",
       " 'number_of_characters': 95,\n",
       " 'max_length': 150,\n",
       " 'dropout_input': 0.3,\n",
       " 'number_of_classes': 9}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterLevelCNN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CharacterLevelCNN, self).__init__()\n",
    "\n",
    "        # define conv layers\n",
    "\n",
    "        self.dropout_input = nn.Dropout2d(args['dropout_input'])\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(args['number_of_characters'],\n",
    "                                             256,\n",
    "                                             kernel_size=7,\n",
    "                                             padding=0),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(3)\n",
    "                                   )\n",
    "\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=7, padding=0),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(3)\n",
    "                                   )\n",
    "\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0),\n",
    "                                   nn.ReLU()\n",
    "                                   )\n",
    "\n",
    "        self.conv4 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0),\n",
    "                                   nn.ReLU()\n",
    "                                   )\n",
    "\n",
    "        self.conv5 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0),\n",
    "                                   nn.ReLU()\n",
    "                                   )\n",
    "\n",
    "        self.conv6 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(3)\n",
    "                                   )\n",
    "\n",
    "        # compute the  output shape after forwarding an input to the conv layers\n",
    "\n",
    "        input_shape = (128,\n",
    "                       args['max_length'],\n",
    "                       args['number_of_characters'])\n",
    "        \n",
    "        self.output_dimension = self._get_conv_output(input_shape)\n",
    "\n",
    "        # define linear layers\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(self.output_dimension, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        self.fc3 = nn.Linear(1024, args['number_of_classes'])\n",
    "\n",
    "        # initialize weights\n",
    "\n",
    "        self._create_weights()\n",
    "\n",
    "    # utility private functions\n",
    "\n",
    "    def _create_weights(self, mean=0.0, std=0.05):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(mean, std)\n",
    "\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        x = torch.rand(shape)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output_dimension = x.size(1)\n",
    "        return output_dimension\n",
    "\n",
    "    # forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout_input(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, args):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.length = len(self.texts)\n",
    "\n",
    "        self.vocabulary = args['alphabet'] \n",
    "        self.number_of_characters = args['number_of_characters'] \n",
    "        self.max_length = args['max_length']\n",
    "        self.identity_mat = np.identity(self.number_of_characters)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raw_text = self.texts[index]\n",
    "\n",
    "        data = np.array([self.identity_mat[self.vocabulary.index(i)] for i in list(raw_text)[::-1] if i in self.vocabulary],\n",
    "                        dtype=np.float32)\n",
    "        if len(data) > self.max_length:\n",
    "            data = data[:self.max_length]\n",
    "        elif 0 < len(data) < self.max_length:\n",
    "            data = np.concatenate(\n",
    "                (data, np.zeros((self.max_length - len(data), self.number_of_characters), dtype=np.float32)))\n",
    "        elif len(data) == 0:\n",
    "            data = np.zeros(\n",
    "                (self.max_length, self.number_of_characters), dtype=np.float32)\n",
    "\n",
    "        label = self.labels[index]\n",
    "        data = torch.Tensor(data)\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(text):\n",
    "    clean_text = re.sub(r'#[A-Za-z0-9_]+', \"\", text)\n",
    "    return clean_text\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    clean_text = re.sub(r'@[A-Za-z0-9_]+', \"\", text)\n",
    "    return clean_text\n",
    "\n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_urls(text):\n",
    "    clean_text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    return clean_text\n",
    "\n",
    "preprocessing_steps = {\n",
    "    'remove_hashtags': remove_hashtags,\n",
    "    'remove_urls': remove_urls,\n",
    "    'remove_user_mentions': remove_user_mentions,\n",
    "    'lower': lower\n",
    "}\n",
    "\n",
    "\n",
    "def process_text(steps, text):\n",
    "    if steps is not None:\n",
    "        for step in steps:\n",
    "            text = preprocessing_steps[step](text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=['remove_hashtags','remove_urls','remove_user_mentions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']=data['text'].apply(lambda text: process_text(steps,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Agent             51.735948\n",
       "Place             18.999886\n",
       "Species            9.087143\n",
       "Work               8.702933\n",
       "Event              7.893961\n",
       "SportsSeason       2.423413\n",
       "UnitOfWork         0.728453\n",
       "TopicalConcept     0.325281\n",
       "Device             0.102981\n",
       "Name: l1, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['l1'].value_counts()/data.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_map(y):\n",
    "    S=set(y)\n",
    "    num=len(S)\n",
    "    dic={word:i for i, word in enumerate(S)}\n",
    "    \n",
    "    labels=y.apply(lambda val: dic[val])\n",
    "    \n",
    "    return labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=labels_map(data['l1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, criterion, optimizer, device):\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train() # Make sure that the model is in training mode.\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # get data\n",
    "            batch_x, batch_y = batch\n",
    "            \n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # get predictions from model\n",
    "            y_pred = model(batch_x)\n",
    "        \n",
    "            # perform backprop\n",
    "            loss = criterion(y_pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "\n",
    "        print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(data['text'].values,labels.values,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train=MyDataset(x_train,y_train,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test=MyDataset(x_test,y_test,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl=torch.utils.data.DataLoader(dataset_test, batch_size=128)\n",
    "train_dl=torch.utils.data.DataLoader(dataset_train, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int,long)): \n",
    "            self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): \n",
    "            self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        if x.dim()>2:\n",
    "            x = x.view(x.size(0),x.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            x = x.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            x = x.contiguous().view(-1,x.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(x)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=x.data.type():\n",
    "                self.alpha = self.alpha.type_as(x.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: \n",
    "            return loss.mean()\n",
    "        else: \n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CharacterLevelCNN(number_classes,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device {}.\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, val_dl, 1, loss_fn,optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(t, 1, torch.tensor([[0,1],[0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
